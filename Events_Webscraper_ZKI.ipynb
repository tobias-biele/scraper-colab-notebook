{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobias-biele/scraper-colab-notebook/blob/main/Events_Webscraper_ZKI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eine Kopie dieses Notebooks ist [hier](https://github.com/tobias-biele/scraper-colab-notebook/blob/main/Events_Webscraper_ZKI.ipynb) gespeichert. Über die Versions-Historie können Änderungen rückgängig gemacht werden."
      ],
      "metadata": {
        "id": "P_3K2vDCby1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importiere die Python-Dateien von Gitlab, installiere Dependencies, lade deutsches Datumsformat"
      ],
      "metadata": {
        "id": "XYd2eyzKbies"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veR-pCbOYJSb",
        "outputId": "83ed1253-79e4-4985-f199-fb346d6870b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'event-scraper'...\n",
            "remote: Enumerating objects: 428, done.\u001b[K\n",
            "remote: Counting objects: 100% (428/428), done.\u001b[K\n",
            "remote: Compressing objects: 100% (131/131), done.\u001b[K\n",
            "remote: Total 428 (delta 289), reused 428 (delta 289), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (428/428), 64.02 KiB | 2.46 MiB/s, done.\n",
            "Resolving deltas: 100% (289/289), done.\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.1.9\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=228ac7d9ebc595f31f5c0523bffaf9cb4cbcf9a08eeec74cd8e71d0948c6d5b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.10 sgmllib3k-1.0.0\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.15.2-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.23.1-py3-none-any.whl (448 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.3/448.3 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.6)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.15.2 trio-0.23.1 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# Backup repository: https://github.com/tobias-biele/event-scraper\n",
        "# Falls https://gitlab.adelphi.app/ offline ist, ersetze die URL in der nächsten Zeile durch die URL aus der vorherigen\n",
        "!git clone https://gitlab.adelphi.app/biele/event-scraper.git\n",
        "!pip install xlsxwriter\n",
        "!pip install feedparser\n",
        "!pip install selenium\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/event-scraper')\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"data\"):\n",
        "  os.makedirs(\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setze die Parameter für den Scraper\n",
        "#### DETAIL_SEITEN_SCRAPEN\n",
        "Die Informationen auf den Kalender-Seiten enthalten häufig nicht alle Informationen. Um auch die Informationen von den Detail-Seiten der Events zu scrapen, setze `DETAIL_SEITEN_SCRAPEN = True`. Dies dauert allerdings etwas länger. Wenn du den Scraper nur schnell testen möchtest und dir die Vollständigkeit der Informationen nicht wichtig ist, setze `DETAIL_SEITEN_SCRAPEN = False`.\n",
        "\n",
        "Da das Schema der Detailseite bekannt sein muss, damit der Scraper funktionieren kann, werden Detailseiten auf manchen Webseiten nicht gescrapt, nämlich dann, wenn die Verlinkungen auf den Events auf externe Webseiten verweisen.\n",
        "\n",
        "#### MINDESTDATUM\n",
        "Alle Events welche vor diesem Datum stattfinden/stattgefunden haben werden nicht berücksichtigt. Du kannst ein konkretes Datum setzen, z. B. `MINDESTDATUM = \"2024-01-01\"` oder das MINDESTDATUM auf `\"heute\"` setzen, so dass alle in der Vergangenheit liegenden Veranstaltungen ignoriert werden. Wenn du `MINDESTDATUM = None` setzt, dann werden alle Veranstaltungen gescrapt, die auf der Seite stehen, auch die welche in der Vergangenheit liegen."
      ],
      "metadata": {
        "id": "JlKpGeJ2YcEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DETAIL_SEITEN_SCRAPEN = True\n",
        "MINDESTDATUM = \"heute\"\n",
        "\n",
        "import locale\n",
        "from parser.utils import today_date_string\n",
        "if MINDESTDATUM == \"heute\":\n",
        "  MINDESTDATUM = today_date_string(reverse_format=True)\n",
        "\n",
        "print(\"Datum, ab dem Events berücksichtigt werden:\", MINDESTDATUM)\n",
        "if DETAIL_SEITEN_SCRAPEN:\n",
        "  print(\"Auch die Detailseiten der einzelnen Events werden gescrapt. Dadurch dauert der Durchlauf etwas länger.\")\n",
        "else:\n",
        "  print(\"Detailseiten der einzelnen Events werden nicht gescrapt.\")"
      ],
      "metadata": {
        "id": "Qt7Iulmvb85q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52846e6-2aaf-4a08-d09c-5affa7ab71f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datum, ab dem Events berücksichtigt werden: 2023-12-04\n",
            "Detailseiten der einzelnen Events werden nicht gescrapt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional: Schränke die Liste der Akteure ein\n",
        "#### INCLUDE\n",
        "Wenn INCLUDE gesetzt wird, dann werden nur Akteure in dieser Liste gescrapt.\n",
        "#### EXCLUDE\n",
        "Alle Akteure in dieser Liste werden nicht gescrapt."
      ],
      "metadata": {
        "id": "Q_aR6dI-EdVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of actors: ['bbsr', 'bfn', 'bmbf', 'bmdv', 'bmel', 'bmuv', 'difu', 'dwd', 'fona', 'hlnug', 'jki', 'lfu_bayern', 'lubw', 'mukmav_saarland', 'mluk_bb', 'niko', 'rlp', 'sachsen', 'tlubn', 'uan', 'uba']\n",
        "INCLUDE = None\n",
        "EXCLUDE = None\n",
        "# INCLUDE = []\n",
        "EXCLUDE = [\"lubw\"]\n"
      ],
      "metadata": {
        "id": "ZHGEabQNEbUQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasse den Scraper laufen"
      ],
      "metadata": {
        "id": "b0lLK_R0b-I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scraper\n",
        "scraper.run(parse_details_pages=DETAIL_SEITEN_SCRAPEN, cut_off_date=MINDESTDATUM, include=INCLUDE, exclude=EXCLUDE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IRN_VIJcAuS",
        "outputId": "76aeaafb-e310-474f-ac60-7a0f22f17efe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exclude list provided. The scraper will not scrape the following actors: ['lubw']\n",
            "Starting...\n",
            "The scraper will not parse details pages.\n",
            "Scraped 4 events from arl (0 skipped)\n",
            "Scraped 3 events from bbsr (1 skipped)\n",
            "Scraped 5 events from bfn (0 skipped)\n",
            "Scraped 0 events from bmbf (7 skipped)\n",
            "Scraped 3 events from bmdv (0 skipped)\n",
            "Scraped 2 events from bmel (0 skipped)\n",
            "Scraped 16 events from bmuv (0 skipped)\n",
            "Scraped 74 events from bund (0 skipped)\n",
            "Scraped 7 events from climate_adapt (2 skipped)\n",
            "Scraped 7 events from cmcc (0 skipped)\n",
            "Scraped 33 events from difu (0 skipped)\n",
            "Scraped 1 events from dkk (1 skipped)\n",
            "Scraped 1 events from dwd (0 skipped)\n",
            "Scraped 7 events from eu_mayors (0 skipped)\n",
            "Scraped 7 events from fona (1 skipped)\n",
            "Scraped 1 events from hlnug (>=1 skipped)\n",
            "Scraped 61 events from idw (401 skipped [including duplicates])\n",
            "Scraped 2 events from jki (0 skipped)\n",
            "Scraped 8 events from lfu_bayern (0 skipped)\n",
            "Skipping lubw because it is in the exclude list.\n",
            "Scraped 15 events from mukmav_saarland (0 skipped)\n",
            "Scraped 4 events from mluk_bb (0 skipped)\n",
            "Scraped 8 events from niko (0 skipped)\n",
            "Scraped 0 events from rlp (1 skipped)\n",
            "Scraped 0 events from sachsen (3 skipped)\n",
            "Scraped 0 events from tlubn (0 skipped)\n",
            "Scraped 33 events from uan (0 skipped)\n",
            "Scraped 12 events from uba (0 skipped)\n",
            "Finished. The result can be found in the data directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtere die gesammelten Events (optional)"
      ],
      "metadata": {
        "id": "bq5w9uZVcAhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setze die Filter-Keywords"
      ],
      "metadata": {
        "id": "fM5wgDcvLBJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORDS = [\n",
        "  \"Hitze\",\n",
        "  \"Starkregen\",\n",
        "  \"Hochwasser\",\n",
        "  \"Trockenheit\",\n",
        "  \"Dürre\",\n",
        "  \"Naturbasiert\",\n",
        "  \"Biodiversität\",\n",
        "  \"Stadtgrün\",\n",
        "  \"Bürgerkommunikation\",\n",
        "  \"Stadtplanung\",\n",
        "  \"Bauleitplanung\",\n",
        "  \"Gesundheit\",\n",
        "  \"Eigenvorsorge\",\n",
        "  \"Verbraucherschutz\",\n",
        "  \"Akteurskommunikation\",\n",
        "  \"Verkehrsplanung\",\n",
        "  \"Straßengestaltung\",\n",
        "]"
      ],
      "metadata": {
        "id": "q4CV5PxtLEYz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wende den Filter an"
      ],
      "metadata": {
        "id": "DHK7cGdOLODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from filter import filter_by_keywords\n",
        "\n",
        "files = os.listdir(\"data\")\n",
        "for file in files:\n",
        "  if file.endswith(\".xlsx\") and not file.endswith(\"_filtered.xlsx\") and not file.endswith(\"_filtered_excluded.xlsx\"):\n",
        "    print(f\"Filtering file {file}\")\n",
        "    filter_by_keywords(file, KEYWORDS)"
      ],
      "metadata": {
        "id": "x_NOg63bogko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4287aaa-d80b-43a9-9502-c7fe7783af53"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering file event_scraper_output_04.12.2023.xlsx\n",
            "Filtering file event_scraper_output_04.12.2023.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4HN0E4BqQQOh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}