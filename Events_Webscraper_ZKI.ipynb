{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobias-biele/scraper-colab-notebook/blob/main/Events_Webscraper_ZKI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importiere die Python-Dateien von Gitlab, installiere Dependencies, lade deutsches Datumsformat"
      ],
      "metadata": {
        "id": "XYd2eyzKbies"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veR-pCbOYJSb",
        "outputId": "81b9e1d7-627b-4cd4-82c0-d3cd6ba149ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'event-scraper'...\n",
            "remote: Enumerating objects: 401, done.\u001b[K\n",
            "remote: Counting objects: 100% (398/398), done.\u001b[K\n",
            "remote: Compressing objects: 100% (392/392), done.\u001b[K\n",
            "remote: Total 401 (delta 275), reused 0 (delta 0), pack-reused 3\u001b[K\n",
            "Receiving objects: 100% (401/401), 58.06 KiB | 2.64 MiB/s, done.\n",
            "Resolving deltas: 100% (275/275), done.\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.1.9\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=5e61b10370e210494be756e3050a1af4ddeff8739e5333f5ae3beae5767e1d27\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.10 sgmllib3k-1.0.0\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.15.2-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.23.1-py3-none-any.whl (448 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.3/448.3 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.3)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.15.2 trio-0.23.1 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# Backup repository: https://github.com/tobias-biele/event-scraper\n",
        "# Falls https://gitlab.adelphi.app/ offline ist, ersetze die URL in der nächsten Zeile durch die URL aus der vorherigen\n",
        "!git clone https://gitlab.adelphi.app/biele/event-scraper.git\n",
        "!pip install xlsxwriter\n",
        "!pip install feedparser\n",
        "!pip install selenium\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/event-scraper')\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"data\"):\n",
        "  os.makedirs(\"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setze die Parameter für den Scraper\n",
        "#### DETAIL_SEITEN_SCRAPEN\n",
        "Die Informationen auf den Kalender-Seiten enthalten häufig nicht alle Informationen. Um auch die Informationen von den Detail-Seiten der Events zu scrapen, setze `DETAIL_SEITEN_SCRAPEN = True`. Dies dauert allerdings etwas länger. Wenn du den Scraper nur schnell testen möchtest und dir die Vollständigkeit der Informationen nicht wichtig ist, setze `DETAIL_SEITEN_SCRAPEN = False`.\n",
        "\n",
        "Da das Schema der Detailseite bekannt sein muss, damit der Scraper funktionieren kann, werden Detailseiten auf manchen Webseiten nicht gescrapt, nämlich dann, wenn die Verlinkungen auf den Events auf externe Webseiten verweisen.\n",
        "\n",
        "#### MINDESTDATUM\n",
        "Alle Events welche vor diesem Datum stattfinden/stattgefunden haben werden nicht berücksichtigt. Du kannst ein konkretes Datum setzen, z. B. `MINDESTDATUM = \"2024-01-01\"` oder das MINDESTDATUM auf `\"heute\"` setzen, so dass alle in der Vergangenheit liegenden Veranstaltungen ignoriert werden. Wenn du `MINDESTDATUM = None` setzt, dann werden alle Veranstaltungen gescrapt, die auf der Seite stehen, auch die welche in der Vergangenheit liegen."
      ],
      "metadata": {
        "id": "JlKpGeJ2YcEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DETAIL_SEITEN_SCRAPEN = True\n",
        "MINDESTDATUM = \"heute\"\n",
        "\n",
        "import locale\n",
        "from parser.utils import today_date_string\n",
        "if MINDESTDATUM == \"heute\":\n",
        "  MINDESTDATUM = today_date_string(reverse_format=True)\n",
        "\n",
        "print(\"heute\", MINDESTDATUM)\n",
        "if DETAIL_SEITEN_SCRAPEN:\n",
        "  print(\"Auch die Detailseiten der einzelnen Events werden gescrapt. Dadurch dauert der Durchlauf etwas länger.\")\n",
        "else:\n",
        "  print(\"Detailseiten der einzelnen Events werden nicht gescrapt.\")"
      ],
      "metadata": {
        "id": "Qt7Iulmvb85q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15ce6c28-95ff-4139-9c80-361f4aeea775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heute 2023-11-30\n",
            "Auch die Detailseiten der einzelnen Events werden gescrapt. Dadurch dauert der Durchlauf etwas länger.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional: Schränke die Liste der Akteure ein\n",
        "#### INCLUDE\n",
        "Wenn INCLUDE gesetzt wird, dann werden nur Akteure in dieser Liste gescrapt.\n",
        "#### EXCLUDE\n",
        "Alle Akteure in dieser Liste werden nicht gescrapt."
      ],
      "metadata": {
        "id": "Q_aR6dI-EdVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wBdd0Y93MefF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of actors: ['bbsr', 'bfn', 'bmbf', 'bmdv', 'bmel', 'bmuv', 'difu', 'dwd', 'fona', 'hlnug', 'jki', 'lfu_bayern', 'lubw', 'mukmav_saarland', 'mluk_bb', 'niko', 'rlp', 'sachsen', 'tlubn', 'uan', 'uba']\n",
        "INCLUDE = None\n",
        "EXCLUDE = None\n",
        "# INCLUDE = []\n",
        "EXCLUDE = [\"lubw\"]\n"
      ],
      "metadata": {
        "id": "ZHGEabQNEbUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lasse den Scraper laufen"
      ],
      "metadata": {
        "id": "b0lLK_R0b-I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scraper\n",
        "scraper.run(parse_details_pages=DETAIL_SEITEN_SCRAPEN, cut_off_date=MINDESTDATUM, include=INCLUDE, exclude=EXCLUDE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IRN_VIJcAuS",
        "outputId": "d185aa77-bf6f-4bc4-f7f4-38492cfeb25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exclude list provided. The scraper will not scrape the following actors: ['lubw']\n",
            "Starting...\n",
            "The scraper will parse details pages for individual events. This might take a while.\n",
            "Scraped 3 events from bbsr (1 skipped)\n",
            "Scraped 4 events from bfn (0 skipped)\n",
            "Scraped 0 events from bmbf (7 skipped)\n",
            "Scraped 4 events from bmdv (0 skipped)\n",
            "Scraped 2 events from bmel (0 skipped)\n",
            "Scraped 17 events from bmuv (0 skipped)\n",
            "Scraped 78 events from bund (1 skipped)\n",
            "Scraped 11 events from climate_adapt (0 skipped)\n",
            "Scraped 13 events from cmcc (0 skipped)\n",
            "Scraped 32 events from difu (0 skipped)\n",
            "Scraped 3 events from dkk (0 skipped)\n",
            "Scraped 0 events from dwd (0 skipped)\n",
            "Scraped 5 events from eu_mayors (0 skipped)\n",
            "Scraped 11 events from fona (0 skipped)\n",
            "Scraped 1 events from hlnug (>=1 skipped)\n",
            "Scraped 2 events from jki (2 skipped)\n",
            "Scraped 9 events from lfu_bayern (0 skipped)\n",
            "Skipping lubw because it is in the exclude list.\n",
            "Scraped 0 events from mukmav_saarland (15 skipped)\n",
            "Scraped 6 events from mluk_bb (0 skipped)\n",
            "Scraped 1 events from niko (0 skipped)\n",
            "Scraped 0 events from rlp (1 skipped)\n",
            "Scraped 0 events from sachsen (3 skipped)\n",
            "Scraped 0 events from tlubn (0 skipped)\n",
            "Scraped 34 events from uan (6 skipped)\n",
            "Scraped 16 events from uba (0 skipped)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtere die gesammelten Events (optional)"
      ],
      "metadata": {
        "id": "bq5w9uZVcAhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setze die Filter-Keywords"
      ],
      "metadata": {
        "id": "fM5wgDcvLBJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KEYWORDS = [\n",
        "  \"Hitze\",\n",
        "  \"Starkregen\",\n",
        "  \"Hochwasser\",\n",
        "  \"Trockenheit\",\n",
        "  \"Dürre\",\n",
        "  \"Naturbasiert\",\n",
        "  \"Biodiversität\",\n",
        "  \"Stadtgrün\",\n",
        "  \"Bürgerkommunikation\",\n",
        "  \"Stadtplanung\",\n",
        "  \"Bauleitplanung\",\n",
        "  \"Gesundheit\",\n",
        "  \"Eigenvorsorge\",\n",
        "  \"Verbraucherschutz\",\n",
        "  \"Akteurskommunikation\",\n",
        "  \"Verkehrsplanung\",\n",
        "  \"Straßengestaltung\",\n",
        "]"
      ],
      "metadata": {
        "id": "q4CV5PxtLEYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wende den Filter an"
      ],
      "metadata": {
        "id": "DHK7cGdOLODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from filter import filter_by_keywords\n",
        "\n",
        "files = os.listdir(\"data\")\n",
        "for file in files:\n",
        "  if file.endswith(\".xlsx\") and not file.endswith(\"_filtered.xlsx\") and not file.endswith(\"_filtered_excluded.xlsx\"):\n",
        "    print(f\"Filtering file {file}\")\n",
        "    filter_by_keywords(file, KEYWORDS)"
      ],
      "metadata": {
        "id": "x_NOg63bogko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "7dfb7d37-3fdd-4e3c-8ceb-99e4871c9982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering file event_scraper_output_30.11.2023.xlsx\n",
            "Error reading Excel file: Worksheet named 'Veranstaltungen' not found\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c3f426ffd9b6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilter_by_keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfilter_by_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: filter_by_keywords() missing 2 required positional arguments: 'filename' and 'keywords'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4HN0E4BqQQOh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}